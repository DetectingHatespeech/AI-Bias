{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Biases Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Using minet to download tweets containing our selection criteria\n",
        "\n",
        "Minet is the Swiss army knife of web mining that was developed at the Medialab of SciencesPo: https://github.com/medialab/minet\n"
      ],
      "metadata": {
        "id": "NdQ0boOJWWr7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rCVOgTgtHcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8147e23a-37e7-4214-e1a4-2512a399d24c"
      },
      "source": [
        "! pip install minet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minet\n",
            "  Downloading minet-0.55.9-py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting colorama>=0.4.0\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting json5>=0.8.5\n",
            "  Downloading json5-0.9.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting browser-cookie3==0.13.0\n",
            "  Downloading browser-cookie3-0.13.0.tar.gz (9.4 kB)\n",
            "Collecting dateparser>=1.0.0\n",
            "  Downloading dateparser-1.1.0-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 42.2 MB/s \n",
            "\u001b[?25hCollecting tenacity>=7.0.0\n",
            "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting persist-queue>=0.7.0\n",
            "  Downloading persist_queue-0.7.0-py2.py3-none-any.whl (32 kB)\n",
            "Collecting ndjson>=0.3.1\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Collecting lxml>=4.3.0\n",
            "  Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from minet) (1.1.0)\n",
            "Collecting casanova<0.18,>=0.17.0\n",
            "  Downloading casanova-0.17.0-py3-none-any.whl (20 kB)\n",
            "Collecting beautifulsoup4>=4.7.1\n",
            "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting ebbe<2,>=1.3.1\n",
            "  Downloading ebbe-1.6.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting ural<0.32,>=0.31.1\n",
            "  Downloading ural-0.31.2-py3-none-any.whl (37 kB)\n",
            "Collecting soupsieve>=2.1\n",
            "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
            "Collecting trafilatura<0.10,>=0.9.1\n",
            "  Downloading trafilatura-0.9.3-py3-none-any.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.7/dist-packages (from minet) (4.62.3)\n",
            "Collecting twitwi<1,>=0.9.2\n",
            "  Downloading twitwi-0.9.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from minet) (3.13)\n",
            "Collecting quenouille<2,>=1.4.2\n",
            "  Downloading quenouille-1.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure]<2,>=1.25.3\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 58.4 MB/s \n",
            "\u001b[?25hCollecting keyring<19.3\n",
            "  Downloading keyring-19.2.0-py2.py3-none-any.whl (34 kB)\n",
            "Collecting cchardet>=2.1.7\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 46.4 MB/s \n",
            "\u001b[?25hCollecting pyaes\n",
            "  Downloading pyaes-1.6.1.tar.gz (28 kB)\n",
            "Collecting pbkdf2\n",
            "  Downloading pbkdf2-1.3.tar.gz (6.4 kB)\n",
            "Collecting lz4\n",
            "  Downloading lz4-3.1.10-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 27.6 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.12.0-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 39.8 MB/s \n",
            "\u001b[?25hCollecting SecretStorage\n",
            "  Downloading SecretStorage-3.3.1-py3-none-any.whl (15 kB)\n",
            "Collecting file-read-backwards<3,>=2.0.0\n",
            "  Downloading file_read_backwards-2.0.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0->minet) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0->minet) (2.8.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0->minet) (1.5.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0->minet) (2019.12.20)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from keyring<19.3->minet) (0.3)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from trafilatura<0.10,>=0.9.1->minet) (2.0.8)\n",
            "Collecting readability-lxml>=0.8.1\n",
            "  Downloading readability_lxml-0.8.1-py3-none-any.whl (20 kB)\n",
            "Collecting courlan>=0.4.2\n",
            "  Downloading courlan-0.6.0-py3-none-any.whl (26 kB)\n",
            "Collecting justext>=3.0.0\n",
            "  Downloading jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from trafilatura<0.10,>=0.9.1->minet) (2021.10.8)\n",
            "Collecting htmldate>=0.9.1\n",
            "  Downloading htmldate-1.0.0-py3-none-any.whl (30 kB)\n",
            "Collecting tld>=0.12\n",
            "  Downloading tld-0.12.6-py37-none-any.whl (412 kB)\n",
            "\u001b[K     |████████████████████████████████| 412 kB 58.8 MB/s \n",
            "\u001b[?25hCollecting langcodes>=3.2.1\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 42.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dateparser>=1.0.0->minet) (1.15.0)\n",
            "Collecting cssselect\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from readability-lxml>=0.8.1->trafilatura<0.10,>=0.9.1->minet) (3.0.4)\n",
            "Collecting pytz\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 56.0 MB/s \n",
            "\u001b[?25hCollecting twitter>=1.19.3\n",
            "  Downloading twitter-1.19.3-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting phylactery<0.3,>=0.2.3\n",
            "  Downloading phylactery-0.2.3-py3-none-any.whl (7.3 kB)\n",
            "Collecting pycountry<19,>=18.12.8\n",
            "  Downloading pycountry-18.12.8-py2.py3-none-any.whl (10.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5 MB 16.9 MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 23.9 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]<2,>=1.25.3->minet) (2.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]<2,>=1.25.3->minet) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]<2,>=1.25.3->minet) (2.21)\n",
            "Collecting jeepney>=0.6\n",
            "  Downloading jeepney-0.7.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: browser-cookie3, pbkdf2, pyaes\n",
            "  Building wheel for browser-cookie3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for browser-cookie3: filename=browser_cookie3-0.13.0-py3-none-any.whl size=8002 sha256=1901ac8430cbcf3fb91f98818b7b2a235bbf55f67b6d53ceadc19be9f03f4fa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c3/49/b4b8995fe98bce9592770ed77dfb8e1e22c0174348626053da\n",
            "  Building wheel for pbkdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pbkdf2: filename=pbkdf2-1.3-py3-none-any.whl size=5103 sha256=1a55c6fd50fd41598e903f29e2e2f12db03c201e098fac98ca3266c44917c2b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/16/ea/daca297d70ee0782ac6e16e83b2c55b2ca42a2113750bc0489\n",
            "  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaes: filename=pyaes-1.6.1-py3-none-any.whl size=26362 sha256=24443f2d12909a09de5fbadb951d2934bc3053d2235feee6be33e3cf5d96bd71\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/a3/7d/73d60820adb58d818179cdec3fc1be66f6f7453513a3ff0b05\n",
            "Successfully built browser-cookie3 pbkdf2 pyaes\n",
            "Installing collected packages: pytz, jeepney, cryptography, urllib3, tld, SecretStorage, pycountry, phylactery, lxml, langcodes, dateparser, cssselect, ural, twitter, soupsieve, readability-lxml, pyOpenSSL, pycryptodome, pyaes, pbkdf2, lz4, keyring, justext, htmldate, file-read-backwards, ebbe, courlan, twitwi, trafilatura, tenacity, quenouille, persist-queue, ndjson, json5, colorama, cchardet, casanova, browser-cookie3, beautifulsoup4, minet\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed SecretStorage-3.3.1 beautifulsoup4-4.10.0 browser-cookie3-0.13.0 casanova-0.17.0 cchardet-2.1.7 colorama-0.4.4 courlan-0.6.0 cryptography-36.0.0 cssselect-1.1.0 dateparser-1.1.0 ebbe-1.6.0 file-read-backwards-2.0.0 htmldate-1.0.0 jeepney-0.7.1 json5-0.9.6 justext-3.0.0 keyring-19.2.0 langcodes-3.3.0 lxml-4.6.4 lz4-3.1.10 minet-0.55.9 ndjson-0.3.1 pbkdf2-1.3 persist-queue-0.7.0 phylactery-0.2.3 pyOpenSSL-21.0.0 pyaes-1.6.1 pycountry-18.12.8 pycryptodome-3.12.0 pytz-2021.3 quenouille-1.4.2 readability-lxml-0.8.1 soupsieve-2.3.1 tenacity-8.0.1 tld-0.12.6 trafilatura-0.9.3 twitter-1.19.3 twitwi-0.9.2 ural-0.31.2 urllib3-1.26.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pytz"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYZz0k69TYWu",
        "outputId": "39f53cd0-7b5c-4db1-a797-36543392dd13"
      },
      "source": [
        "! minet twitter scrape --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: minet twitter scrape\n",
            "       [-h]\n",
            "       [--include-refs]\n",
            "       [-l LIMIT]\n",
            "       [-o OUTPUT]\n",
            "       [--query-template QUERY_TEMPLATE]\n",
            "       [-s SELECT]\n",
            "       {tweets}\n",
            "       query\n",
            "       [file]\n",
            "\n",
            "Minet Twitter Scrape Command\n",
            "============================\n",
            "\n",
            "Scrape Twitter's public facing search API to collect tweets etc.\n",
            "\n",
            "Be sure to check Twitter's advanced search to check what kind of\n",
            "operators you can use to tune your queries (time range, hashtags,\n",
            "mentions, boolean etc.):\n",
            "https://twitter.com/search-advanced?f=live\n",
            "\n",
            "Useful operators include \"since\" and \"until\" to search specific\n",
            "time ranges like so: \"since:2014-01-01 until:2017-12-31\".\n",
            "\n",
            "positional arguments:\n",
            "  {tweets}\n",
            "    What to scrape. Currently only `tweets` is possible.\n",
            "  query\n",
            "    Search query or name of the column containing queries to run in given CSV file.\n",
            "  file\n",
            "    Optional CSV file containing the queries to be run.\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "    show this help message and exit\n",
            "  --include-refs\n",
            "    Whether to emit referenced tweets (quoted, retweeted & replied) in the CSV output. Note that it consumes a memory proportional to the total number of unique tweets retrieved.\n",
            "  -l LIMIT, --limit LIMIT\n",
            "    Maximum number of tweets to collect per query.\n",
            "  -o OUTPUT, --output OUTPUT\n",
            "    Path to the output file. By default, the results will be printed to stdout.\n",
            "  --query-template QUERY_TEMPLATE\n",
            "    Query template. Can be useful for instance to change a column of twitter user screen names into from:@user queries.\n",
            "  -s SELECT, --select SELECT\n",
            "    Columns of input CSV file to include in the output (separated by `,`).\n",
            "\n",
            "examples:\n",
            "\n",
            ". Collecting the latest 500 tweets of a given Twitter user:\n",
            "    $ minet tw scrape tweets \"from:@jack\" --limit 500 > tweets.csv\n",
            "\n",
            ". Collecting the tweets from multiple Twitter queries listed in a CSV file:\n",
            "    $ minet tw scrape tweets query queries.csv > tweets.csv\n",
            "\n",
            ". Templating the given CSV column to query tweets by users:\n",
            "    $ minet tw scrape tweets user users.csv --query-template 'from:@{value}' > tweets.csv\n",
            "\n",
            ". Tip: You can add a \"OR @aNotExistingHandle\" to your query to avoid searching\n",
            "  for your query terms in usernames or handles.\n",
            "  Note that this is a temporary hack which might stop working at any time so be\n",
            "  sure to double check before relying on this trick.\n",
            "  For more information see the related discussion here:\n",
            "  https://webapps.stackexchange.com/questions/127425/how-to-exclude-usernames-and-handles-while-searching-twitter\n",
            "\n",
            "    $ minet tw scrape tweets \"keyword OR @anObviouslyNotExistingHandle\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Minet allows us to filter and search for tweets using the advanced search criterias of Twitter\n",
        "\n",
        "These include \n",
        "1.   A list of Hashtags, where at least one of them has to included in the tweet\n",
        "2. The language the tweet is written in\n",
        "3. If the tweet contains external links or images\n",
        "4. If it is an original tweet or if a reply to another tweet\n",
        "\n",
        "We also aplied a limit of 25000 tweets to our data collection to not overwhelm our analytical capabilities\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rlJt7AgTV_33"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ltT7MKgrs4K"
      },
      "source": [
        "! minet twitter scrape tweets \"(#lgbt OR #gay OR #pride OR #lesbian OR #queer) lang:en -filter:links -filter:replies\"  --limit 25000 > LGBT.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3ZLbMLdsl6X"
      },
      "source": [
        "! minet twitter scrape tweets \"(#blacklivesmatter OR #problack OR #blackandproud OR #blackpride OR #blackempowerment) -filter:links -filter:replies\"  --limit 25000 > Racial.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvzRXjisseGn"
      },
      "source": [
        "! minet twitter scrape tweets \"(#effyourbeautystandards OR #bodypositive OR #celebratemysize OR #honormycurves OR #allbodiesarebeautiful) lang:en -filter:links -filter:replies\"  --limit 25000 > Bodypositiv.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv24AUJTsnmp"
      },
      "source": [
        "! minet twitter scrape tweets \"(#feminism OR #womenrights OR #womenempowerment OR #mydressmychoice OR #ShoutYourAbortion) lang:en -filter:links -filter:replies\"  --limit 25000 > Women.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTzhge9Clacj"
      },
      "source": [
        "# Scattertext for Wordvisualisation\n",
        "\n",
        "The tool scattertext allows for interactive, effective and comprehensive text visualisations. \n",
        "\n",
        "The following code was used to create the .html files of the scattertext plots displayed on the Github page and in the repository. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGaYTM0LmD4H",
        "outputId": "23e1b8df-c45d-481b-863a-7c048d1ce1f5"
      },
      "source": [
        "!pip install scattertext\n",
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scattertext\n",
            "  Downloading scattertext-0.1.5-py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting gensim>=4.0.0\n",
            "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.0.1)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scattertext) (1.1.5)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from scattertext) (0.10.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->scattertext) (5.2.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scattertext) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scattertext) (3.0.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->scattertext) (0.5.2)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9310 sha256=52ebc336ebb32d37ac177363b53000b0b285d91e1470ff66568d426ee2565cb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: mock, gensim, flashtext, scattertext\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed flashtext-2.7 gensim-4.1.2 mock-4.0.3 scattertext-0.1.5\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7dm2aSmlZRz"
      },
      "source": [
        "import scattertext as st\n",
        "import pandas as pd\n",
        "import spacy as spacy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWELIU-5Zn-3",
        "outputId": "ad8cbc11-5d80-4ee5-f9ec-12c2737d6e3f"
      },
      "source": [
        "df = pd.read_excel(\"/content/Final_LGBTQ_clean.xlsx\")\n",
        "df.dtypes\n",
        "df.astype({'Dummy': 'string'}).dtypes\n",
        "df.astype({'text': 'string'}).dtypes\n",
        "df=df.replace({0:\"non-toxic\",1:\"toxic\"})\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "corpus = st.CorpusFromPandas(\n",
        "    df, category_col=\"Dummy\", text_col=\"text\", nlp=nlp\n",
        ").build()\n",
        "\n",
        "sent = st.produce_scattertext_explorer(\n",
        "    corpus,\n",
        "    category=\"toxic\",\n",
        "    category_name=\"toxic\",\n",
        "    not_category_name=\"non-toxic\",\n",
        "    width_in_pixels=1000,\n",
        ")\n",
        "\n",
        "open('./toxic_LGBTQ_FINAL.html', 'w').write(sent)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6878516"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/final_Women_clean.xlsx\")\n",
        "df.dtypes\n",
        "df.astype({'Dummy': 'string'}).dtypes\n",
        "df.astype({'text': 'string'}).dtypes\n",
        "df=df.replace({0:\"non-toxic\",1:\"toxic\"})\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "corpus = st.CorpusFromPandas(\n",
        "    df, category_col=\"Dummy\", text_col=\"text\", nlp=nlp\n",
        ").build()\n",
        "\n",
        "sent = st.produce_scattertext_explorer(\n",
        "    corpus,\n",
        "    category=\"toxic\",\n",
        "    category_name=\"toxic\",\n",
        "    not_category_name=\"non-toxic\",\n",
        "    width_in_pixels=1000,\n",
        ")\n",
        "\n",
        "open('./toxic_Women_FINAL.html', 'w').write(sent)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ba5Rqo4uANK",
        "outputId": "784f1e71-ee4e-43f0-fe83-9070e3bec458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7001937"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/final_bodypositive_clean.xlsx\")\n",
        "df.dtypes\n",
        "df.astype({'Dummy': 'string'}).dtypes\n",
        "df.astype({'text': 'string'}).dtypes\n",
        "df=df.replace({0:\"non-toxic\",1:\"toxic\"})\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "corpus = st.CorpusFromPandas(\n",
        "    df, category_col=\"Dummy\", text_col=\"text\", nlp=nlp\n",
        ").build()\n",
        "\n",
        "sent = st.produce_scattertext_explorer(\n",
        "    corpus,\n",
        "    category=\"toxic\",\n",
        "    category_name=\"toxic\",\n",
        "    not_category_name=\"non-toxic\",\n",
        "    width_in_pixels=1000,\n",
        ")\n",
        "\n",
        "open('./toxic_Bodypositive_FINAL.html', 'w').write(sent)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHVYQ6EB8rvU",
        "outputId": "ed7248e4-bddf-4fe5-b167-fe144a65458f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4904475"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/final_racial_clean.xlsx\")\n",
        "df.dtypes\n",
        "df.astype({'Dummy': 'string'}).dtypes\n",
        "df.astype({'text': 'string'}).dtypes\n",
        "df=df.replace({0:\"non-toxic\",1:\"toxic\"})\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "corpus = st.CorpusFromPandas(\n",
        "    df, category_col=\"Dummy\", text_col=\"text\", nlp=nlp\n",
        ").build()\n",
        "\n",
        "sent = st.produce_scattertext_explorer(\n",
        "    corpus,\n",
        "    category=\"toxic\",\n",
        "    category_name=\"toxic\",\n",
        "    not_category_name=\"non-toxic\",\n",
        "    width_in_pixels=1000,\n",
        ")\n",
        "\n",
        "open('./toxic_Racial_FINAL.html', 'w').write(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uedMCc938r8v",
        "outputId": "a2e5c578-903a-47d0-8d8a-24d8dd34394e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5199551"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}